{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbatim analysis (using word cloud and Ngram )\n",
    "\n",
    "## ====================================================\n",
    "\n",
    "#### Author : Shivani\n",
    "#### last updated : 16-09-2020\n",
    "\n",
    "## ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataset \n",
    "data_all = pd.read_excel(\"Comment_analysis_lockdown.xlsx\", sheet_name = \"Data_lockdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filt = data_all.drop_duplicates()\n",
    "data_filt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filt[\"Agent_related\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering for comments with greater than 100 words \n",
    "data_ld = data_filt[data_filt[\"Agent_related\"].isin([\"yes\",\"Yes\"])]\n",
    "\n",
    "#filtering for lockdown period\n",
    "data_ld = data_ld[data_ld[\"response_phase\"] == \"Lockdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ld.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ld[data_ld['csat_flag'] == 0 ]['journeynode'].value_counts()\n",
    "\n",
    "# limit of 50 count, remove others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ld[(data_ld['csat_flag'] == 0) & (data_ld['csat_comments'].str.contains('skill')) ]['journeynode'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE PROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates and saving into new variable \n",
    "documents = data_ld['csat_comments'].drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting it to a dataframe \n",
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all symbols etc from the comments \n",
    "\n",
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting mis splet words\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: reduce_lengthening(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "news_df['text_lemmatized'] = news_df.clean_doc.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "processed_text = news_df[\"text_lemmatized\"]\n",
    "\n",
    "for i in range(len(processed_text)):\n",
    "    tokens = processed_text.iloc[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in DF:\n",
    "    DF[i] = len(DF[i])\n",
    "# DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = [x for x in DF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(news_df)\n",
    "total_vocab_size = len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating TD IDF \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "doc = 0\n",
    "\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text.iloc[i]\n",
    "    print(tokens)\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        print(counter)\n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Merging the td idf according to weigths \n",
    "\n",
    "# alpha = 0.3\n",
    "\n",
    "# for i in tf_idf:\n",
    "#     tf_idf[i] *= alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(columns=['keys', 'values'])\n",
    "a['keys'] = tf_idf.keys()\n",
    "a['values'] = tf_idf.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['keys'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"words\"] = pd.DataFrame(a['keys'].values.tolist(), index=a.index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a[\"values\"] > a[\"values\"].quantile(0.90)]['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter for records more than 30\n",
    "data_ld_top_jn = data_ld[data_ld['journeynode'].isin(['PRE DELIVERY','REFUNDS','ORDER MODIFICATION','PAYMENTS','RETURNS',\n",
    "                                               'PRE PURCHASE','OFFERS AND PROMOTIONS','POST DELIVERY'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOURNEY NODE FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ld_top_jn['journeynode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ld_jn = data_ld_top_jn[(data_ld_top_jn['journeynode'] == \"PAYMENTS\") & (data_ld_top_jn[\"csat_flag\"] == 0)]\n",
    "data_ld_jn['journeynode'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_ld_jn.to_csv(\"JN_Data\\\\Data_POST DELIVERY.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicates and saving into new variable \n",
    "documents = data_ld_jn['csat_comments'].drop_duplicates().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting it to a dataframe \n",
    "news_df = pd.DataFrame({'document':documents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all symbols etc from the comments \n",
    "\n",
    "# removing everything except alphabets`\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n]\", \" \")\n",
    "\n",
    "# removing short words\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting mis splet words\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: reduce_lengthening(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# import nltk\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "news_df['text_lemmatized'] = news_df.clean_doc.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"tokens\"] = news_df[\"clean_doc\"].apply( lambda x : word_tokenize(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer=PorterStemmer()\n",
    "\n",
    "# def stem_sentences(sentence):\n",
    "#     tokens = sentence.split()\n",
    "#     stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#     return ' '.join(stemmed_tokens)\n",
    "\n",
    "# news_df['Stem_words'] = news_df['text_lemmatized'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem(\"really\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD CLOUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_df[\"text_lemmatized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the journey node sentence \n",
    "list_words = news_df[\"text_lemmatized\"]\n",
    "\n",
    "#storing the words from tf_idf \n",
    "in_tf_idf = a[a[\"values\"] > a[\"values\"].quantile(0.75)][\"words\"].tolist()\n",
    "\n",
    "#words are present in the top quartile of tf_idf\n",
    "\n",
    "final_list_words = []\n",
    "\n",
    "for i in list_words:\n",
    "#     print(i)\n",
    "    final_list_words.extend([word for word in i if word in in_tf_idf])\n",
    "\n",
    "# print(final_list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the final list of filtered words \n",
    "\n",
    "all_headlines = ' '.join(x.lower() for x in final_list_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = STOPWORDS\n",
    "# 'flipkart'\n",
    "l = ['flipkart','customer','support','service','customer care','will','executive','agent','care','issue','still','even',\n",
    "    'representative','agents','consultant','till','call','problem','return', 'order', 'product','please','give','delivery',\n",
    "    'reply','time','response','without','chat','resolved','resolution','feedback','team','worst','poor','deliver',\n",
    "    'delivered','email','gives','payment','later','option','solve','help','good','resolve','sending','card','solution']\n",
    "\n",
    "for i in l:\n",
    "    stopwords.add(i)\n",
    "    \n",
    "\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=60).generate(all_headlines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 15, 25\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in all_headlines.split() if word not in stopwords]\n",
    "counted_words = collections.Counter(filtered_words)\n",
    "\n",
    "words = []\n",
    "counts = []\n",
    "for letter, count in counted_words.most_common(10):\n",
    "    words.append(letter)\n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "plt.title('Top words in the headlines vs their count')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.barh(words, counts, color=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 1 - NGRAM & POLARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word', stop_words=stopwords)\n",
    "\n",
    "sparse_matrix = word_vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "\n",
    "pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "temp1 = temp.sort_values( by = \"frequency\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1[temp1['frequency'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1.index.name = 'phrases'\n",
    "temp1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp1.to_csv(\"Ngram_resultv2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = news_df[news_df['document'].str.contains('understand')]\n",
    "df = df[['document']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['document'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "df['review_len'] = df['document'].astype(str).apply(len)\n",
    "df['word_count'] = df['document'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(df['polarity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.to_csv(\"polarity.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('5 random reviews with the highest positive sentiment polarity: \\n')\n",
    "# cl = df.loc[df.polarity >= 0.9, ['clean_doc']].sample(5).values\n",
    "# for c in cl:\n",
    "#     print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 random reviews with the most neutral sentiment(zero) polarity: \\n')\n",
    "cl = df.loc[df.polarity == 0, ['document']].sample(3).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('5 reviews with the most negative polarity: \\n')\n",
    "cl = df.loc[df.polarity <= -0, ['document']].sample(5).values\n",
    "for c in cl:\n",
    "    print(c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
